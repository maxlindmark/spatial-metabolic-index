---
title: "Collate CPUE data"
author: "Max Lindmark"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
  df_print: paged
  code_folding: hide
pdf_document: default
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align ='center'
)
```

# Intro
In this script, I load exchange data from datras and calculate catch of cod and flounder in unit kg/km^2 (with TVL gear) by size group, by correcting for gear dimensions, sweeplength and trawl speed, following Orio et al 2017. 

## Load libraries

```{r, message=FALSE}
library(tidyverse)
library(readxl)
library(tidylog)
library(RCurl)
library(viridis)
library(RColorBrewer)
library(patchwork)
library(janitor)
library(icesDatras)
library(mapdata)
library(patchwork)
library(rgdal)
library(raster)
library(sf)
library(rgeos)
library(chron)
library(lattice)
library(ncdf4)
library(marmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(mapplots)
library(geosphere)
library(modelr)
library(devtools)

world <- ne_countries(scale = "medium", returnclass = "sf")

# Source code for map plots
source_url("https://raw.githubusercontent.com/maxlindmark/spatial-metabolic-index/main/R/functions/map-plot.R")

# Source code for lon lat to utm
source_url("https://raw.githubusercontent.com/maxlindmark/spatial-metabolic-index/main/R/functions/lon-lat-utm.R")

theme_set(theme_plot())
```

## Read data

```{r, message=FALSE}
# Data were read in from getDATRAS on 2022.09.06
# Read HH data
# bits_hh <- getDATRAS(record = "HH", survey = "BITS", years = 1991:2020, quarters = c(1, 4))
# write.csv(bits_hh, "data/DATRAS_exchange/bits_hh.csv")
bits_hh <- read.csv("data/DATRAS_exchange/bits_hh.csv") %>% filter(Year > 1992) # To match covariates

# Read HL data
# bits_hl <- getDATRAS(record = "HL", survey = "BITS", years = 1991:2020, quarters = c(1, 4))
# write.csv(bits_hl, "data/DATRAS_exchange/bits_hl.csv")
bits_hl <- read.csv("data/DATRAS_exchange/bits_hl.csv") %>% filter(Year > 1992) # To match covariates

# Read CA data
# bits_ca <- getDATRAS(record = "CA", survey = "BITS", years = 1991:2020, quarters = c(1, 4))
# write.csv(bits_ca, "data/DATRAS_exchange/bits_ca.csv")
bits_ca <- read.csv("data/DATRAS_exchange/bits_ca.csv") %>% filter(Year > 1992) # To match covariates

# Read gear standardization data 
sweep <- read.csv("data/gear_standardization/sweep_9116.csv", sep = ";", dec = ",", fileEncoding = "latin1")
sweep <- read.csv("data/gear_standardization/sweep_9118_ml.csv", sep = ";", fileEncoding = "latin1")
```

## Standardize catch data
### Standardize ships
```{r, message=FALSE}
# Before creating a a new ID, make sure that countries and ships names use the same format
sort(unique(sweep$Ship))
sort(unique(bits_hh$Ship))
sort(unique(bits_hl$Ship))

# Change back to the old Ship name standard...
# https://vocab.ices.dk/?ref=315
# https://vocab.ices.dk/?ref=315
# Assumptions:
# SOL is Solea on ICES links above, and SOL1 is the older one of the two SOLs (1 and 2)
# DAN is Dana
# sweep %>% filter(Ship == "DANS") %>% distinct(Year, Country)
# sweep %>% filter(Ship == "DAN2") %>% distinct(Year)
# bits_hh %>% filter(Ship == "67BC") %>% distinct(Year, Country)
# sweep %>% filter(Ship == "DAN2") %>% distinct(Year)
# bits_hh %>% filter(Ship == "26D4") %>% distinct(Year) # Strange that 26DF doesn't extend far back. Which ship did the Danes use? Ok, I have no Danish data that old.
# bits_hh %>% filter(Country == "DK") %>% distinct(Year)

bits_hh <- bits_hh %>%
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) %>% 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

bits_hl <- bits_hl %>%
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) %>% 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

bits_ca <- bits_ca %>%
  mutate(Ship2 = fct_recode(Ship,
                            "SOL" = "06S1", 
                            "SOL2" = "06SL",
                            "DAN2" = "26D4",
                            "HAF" = "26HF",
                            "HAF" = "26HI",
                            "HAF" = "67BC",
                            "BAL" = "67BC",
                            "ARG" = "77AR",
                            "77SE" = "77SE",
                            "AA36" = "AA36",
                            "KOOT" = "ESLF",
                            "KOH" = "ESTM",
                            "DAR" = "LTDA",
                            "ATLD" = "RUJB",
                            "ATL" = "RUNT"), 
         Ship2 = as.character(Ship2)) %>% 
  mutate(Ship3 = ifelse(Country == "LV" & Ship2 == "BAL", "BALL", Ship2))

# Ok, which ships are missing in the exchange data?
unique(bits_hh$Ship3)[!unique(bits_hh$Ship3) %in% unique(sweep$Ship)]
# Swedish Ships and unidentified ships are NOT in the Sweep data
unique(sweep$Ship3)[!unique(sweep$Ship3) %in% unique(bits_hh$Ship3)]
# But all Sweep Ships are in the exchange data
```

### Standardize countries

```{r, message=FALSE}
# Now check which country codes are used
sort(unique(sweep$Country))
sort(unique(bits_hh$Country))

# https://www.nationsonline.org/oneworld/country_code_list.htm#E
bits_hh <- bits_hh %>%
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

bits_hl <- bits_hl %>%
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

bits_ca <- bits_ca %>%
  mutate(Country = fct_recode(Country,
                              "DEN" = "DK",
                              "EST" = "EE",
                              "GFR" = "DE",
                              "LAT" = "LV",
                              "LTU" = "LT",
                              "POL" = "PL",
                              "RUS" = "RU",
                              "SWE" = "SE"),
         Country = as.character(Country))

# Gear? Are they the same?
sort(unique(bits_hh$Gear))
sort(unique(bits_hl$Gear))
sort(unique(sweep$Gear))

# Which gears are NOT in the sweep data?
unique(bits_hl$Gear)[!unique(bits_hl$Gear) %in% unique(sweep$Gear)] 
```

### Create a simple haul ID that works across all exchange data

```{r, message=FALSE}
# Create ID column
bits_ca <- bits_ca %>% 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_hl <- bits_hl %>% 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_hh <- bits_hh %>% 
  mutate(IDx = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

# Works like a haul-id
bits_hh %>% group_by(IDx) %>% mutate(n = n()) %>% ungroup() %>% distinct(n)
```

### Create the same unique haul-ID in the cpue data that I have in the sweep-file

```{r, message=FALSE}
bits_hl <- bits_hl %>% 
  mutate(haul.id = paste(Year, Quarter, Country, Ship3, Gear, StNo, HaulNo, sep = ":")) 

bits_hh <- bits_hh %>% 
  mutate(haul.id = paste(Year, Quarter, Country, Ship3, Gear, StNo, HaulNo, sep = ":")) 

bits_hh %>% group_by(haul.id) %>% mutate(n = n()) %>% ungroup() %>% distinct(n)
```

### Clean DATRAS EXCHANGE data

```{r, message=FALSE}
# Select just valid, additional and no oxygen hauls
bits_hh <- bits_hh %>%
  #filter(!Country == "SWE") %>% # I'll deal with Sweden later...
  filter(HaulVal %in% c("A","N","V"))

# Add ICES rectangle
bits_hh$Rect <- mapplots::ices.rect2(lon = bits_hh$ShootLong, lat = bits_hh$ShootLat)

# Add ICES subdivisions
shape <- shapefile("data/ICES_StatRec_mapto_ICES_Areas/StatRec_map_Areas_Full_20170124.shp")

pts <- SpatialPoints(cbind(bits_hh$ShootLong, bits_hh$ShootLat), 
                     proj4string = CRS(proj4string(shape)))

bits_hh$sub_div <- over(pts, shape)$Area_27

# Rename subdivisions to the more common names and do some more filtering (by sub div and area)
sort(unique(bits_hh$sub_div))

bits_hh <- bits_hh %>% 
  mutate(sub_div = factor(sub_div),
         sub_div = fct_recode(sub_div,
                              "20" = "3.a.20",
                              "21" = "3.a.21",
                              "22" = "3.c.22",
                              "23" = "3.b.23",
                              "24" = "3.d.24",
                              "25" = "3.d.25",
                              "26" = "3.d.26",
                              "27" = "3.d.27",
                              "28" = "3.d.28.1",
                              "28" = "3.d.28.2",
                              "29" = "3.d.29"),
         sub_div = as.character(sub_div)) 

# Now add the fishing line information from the sweep file (we need that later
# to standardize based on gear geometry). We add in the the HH data and then
# transfer it to the other exchange data files when left_joining.
# Check which Fishing lines I have in the sweep data:
fishing_line <- sweep %>% group_by(Gear) %>% distinct(Fishing.line)

bits_hh <- left_join(bits_hh, fishing_line)
# sweep %>% group_by(Gear) %>% distinct(Fishing.line)
# bits_hh %>% group_by(Gear) %>% distinct(Fishing.line)
bits_hh$Fishing.line <- as.numeric(bits_hh$Fishing.line)

# Which gears do now have fishing line?
bits_hh$Fishing.line[is.na(bits_hh$Fishing.line)] <- -9
bits_hh %>% filter(Fishing.line == -9) %>% distinct(Gear)
# 1  GRT
# 2  CAM
# 3  EXP
# 4  FOT
# 5  GOV
# 6  EGY
# 7   DT
# 8  ESB
# 9  HAK

# FROM the index files (Orio, "Research Östersjön 2")
# FOT has 83
# GOV has 160
# ESB ??
# GRT ??
# Rest are unknown and likely not used by Swedish data (therefore their correction
# factors my be in the sweep file)

# Add these values:
bits_hh <- bits_hh %>% mutate(Fishing.line = ifelse(Gear == "FOT", 83, Fishing.line))
bits_hh <- bits_hh %>% mutate(Fishing.line = ifelse(Gear == "GOV", 160, Fishing.line))

# Now select the hauls in the HH data when subsetting the HL data
bits_hl <- bits_hl %>%
  filter(haul.id %in% bits_hh$haul.id)

# Match columns from the HH data to the HL and CA data
sort(unique(bits_hh$sub_div))
sort(colnames(bits_hh))

# No NAs for the variables going in to the stomach haul ID
unique(is.na(bits_hh %>% dplyr::select(Year, Quarter, Month, Country, Rect, HaulNo)))

# Before making the id_haul_stomach variable we need to change the country column so that it actually matches the stomach data
# This is the stomach version:
#[1] "LV" "PL" "SE" "DK"
unique(bits_hh$Country)

# MAKE SURE THE COUNTRY CODE IS THE SAME! FOR NOW I DON*T USE COUNTRY 2
bits_hh <- bits_hh %>% mutate(Country2 = NA,
                              Country2 = ifelse(Country == "LAT", "LV", Country2),
                              Country2 = ifelse(Country == "POL", "PL", Country2),
                              Country2 = ifelse(Country == "SWE", "SE", Country2),
                              Country2 = ifelse(Country == "DEN", "DK", Country2))

bits_hh_merge <- bits_hh %>% 
  mutate(id_haul_stomach = paste(Year, Quarter, Month, Country, Rect, HaulNo, sep = ".")) %>% 
  dplyr::select(sub_div, Rect, HaulVal, StdSpecRecCode, BySpecRecCode, Fishing.line, Month,
                DataType, HaulDur, GroundSpeed, haul.id, IDx, ShootLat, ShootLong, id_haul_stomach)

bits_hl <- left_join(dplyr::select(bits_hl, -haul.id), bits_hh_merge, by = "IDx")
bits_ca <- left_join(bits_ca, bits_hh_merge, by = "IDx")

# Now filter the subdivisions I want from all data sets
bits_hh <- bits_hh %>% filter(sub_div %in% c(24, 25, 26, 27, 28))
bits_hl <- bits_hl %>% filter(sub_div %in% c(24, 25, 26, 27, 28))
bits_ca <- bits_ca %>% filter(sub_div %in% c(24, 25, 26, 27, 28))
```

```{r}
bits_hl %>% filter(Year == 2016 & Quarter == 1 & Month == 2 & Country == "SWE" & Rect == "39G4") %>% distinct(HaulNo)
```

### Filter species

```{r, test}
bits_hl %>% 
  #filter(SpecCodeType == "W") %>% 
  group_by(haul.id, SpecCode) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  group_by(SpecCode) %>% 
  summarise(n = sum(n)) %>% 
  filter(n > 2000) %>% 
  mutate(species = SpecCode,
         species = ifelse(SpecCode %in% c("126417", "161722"), "herring", species),
         species = ifelse(SpecCode %in% c("126425", "161789"), "sprat", species),
         species = ifelse(SpecCode %in% c("126438", "164758"), "whiting", species),
         species = ifelse(SpecCode %in% c("126450", "164748"), "fourbeard_rockling", species),
         species = ifelse(SpecCode %in% c("126736"), "smelt", species),
         species = ifelse(SpecCode %in% c("127123"), "eelpout", species),
         species = ifelse(SpecCode %in% c("127139", "172881"), "dab", species),
         species = ifelse(SpecCode %in% c("127143", "172902"), "plaice", species),
         species = ifelse(SpecCode %in% c("127141", "172894"), "flounder", species),
         species = ifelse(SpecCode %in% c("127149", "616195"), "turbot", species),
         species = ifelse(SpecCode %in% c("127203"), "shorthorn_sculpin", species),
         species = ifelse(SpecCode %in% c("126436", "164712"), "cod", species)
         ) %>% 
  ggplot() +
  geom_bar(aes(reorder(species, desc(n)), n), stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, size = 11)) + 
  labs(x = "")

# Based on this plot, and species we have metabolic index parameters for, we will go ahead with the following species:
# Cod, flounder, plaice, dab


# Plot some species in space!
bits_hl %>% 
  mutate(species = NA,
         species = ifelse(SpecCode %in% c("126436", "164712"), "cod", species),
         species = ifelse(SpecCode %in% c("127141", "172894"), "flounder", species),
         species = ifelse(SpecCode %in% c("127143", "172902"), "plaice", species),
         species = ifelse(SpecCode %in% c("127139", "172881"), "dab", species)) %>% 
  drop_na(species) %>% 
  group_by(Year, haul.id, ShootLong, ShootLat, species) %>% 
  summarise(tot_no = sum(TotalNo)) %>% 
  filter(tot_no > 0) %>% 
  filter(ShootLong > 13.5) %>% 
  ggplot(aes(ShootLong, ShootLat, color = log(tot_no))) + 
  geom_point() +
  facet_grid(species ~ Year)
```

```{r, message=FALSE}
hlcod <- bits_hl %>%
  filter(SpecCode %in% c("126436", "164712")) %>% 
  mutate(Species = "Gadus morhua")

hlfle <- bits_hl %>%
  filter(SpecCode %in% c("127141", "172894")) %>% 
  mutate(Species = "Platichthys flesus")

hlpla <- bits_hl %>%
  filter(SpecCode %in% c("127143", "172902")) %>%
  mutate(Species = "Pleuronectes platessa")

hldab <- bits_hl %>%
  filter(SpecCode %in% c("127139", "172881")) %>% 
  mutate(Species = "Limanda limanda")
```

### Prepare to add 0 catches

```{r, message=FALSE}
# Find common columns in the HH and HL data (here already subset by species)
comcol <- intersect(names(hlcod), names(bits_hh))

# What is the proportion of zero-catch hauls?
# Here we don't have zero catches
hlcod %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(HLNoAtLngt)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>% 
  distinct(zero_catch)

# Cod: Add 0s and then remove lines with SpecVal = 0 (first NA because we don't have a match in the HH, then make them 0 later)
hlcod0 <- full_join(hlcod, bits_hh[, comcol], by = comcol)

# No zeroes yet
hlcod0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(HLNoAtLngt)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>% 
  distinct(zero_catch) 

hlcod0$SpecVal[is.na(hlcod0$SpecVal)] <- "zeroCatch"

hlcod0$SpecVal <- factor(hlcod0$SpecVal)

hlcod0 <- hlcod0 %>% filter(!SpecVal == "0")

# Add species again after merge
hlcod0$Species <- "Gadus morhua"

# Flounder: Add 0s, remove them if StdSpecRecCode !=1 and then remove lines with SpecVal = 0
hlfle0 <- full_join(hlfle, bits_hh[, comcol], by = comcol)

hlfle0 <- hlfle0[!(is.na(hlfle0$Species) & hlfle0$StdSpecRecCode != 1), ] 

hlfle0$SpecVal[is.na(hlfle0$SpecVal)] <- "zeroCatch"
hlfle0$SpecVal <- factor(hlfle0$SpecVal)

hlfle0 <- hlfle0 %>% filter(!SpecVal == "0")

hlfle0$Species<-"Platichthys flesus"

# Plaice: Add 0s, remove them if StdSpecRecCode !=1 and then remove lines with SpecVal = 0
hlpla0 <- full_join(hlpla, bits_hh[, comcol], by = comcol)

hlpla0 <- hlpla0[!(is.na(hlpla0$Species) & hlpla0$StdSpecRecCode != 1), ] 

hlpla0$SpecVal[is.na(hlpla0$SpecVal)] <- "zeroCatch"
hlpla0$SpecVal <- factor(hlpla0$SpecVal)

hlpla0 <- hlpla0 %>% filter(!SpecVal == "0")

hlpla0$Species<-"Pleuronectes platessa"

# Dab: Add 0s, remove them if StdSpecRecCode !=1 and then remove lines with SpecVal = 0
hldab0 <- full_join(hldab, bits_hh[, comcol], by = comcol)

hldab0 <- hldab0[!(is.na(hldab0$Species) & hldab0$StdSpecRecCode != 1), ] 

hldab0$SpecVal[is.na(hldab0$SpecVal)] <- "zeroCatch"
hldab0$SpecVal <- factor(hldab0$SpecVal)

hldab0 <- hldab0 %>% filter(!SpecVal == "0")

hldab0$Species<-"Limanda limanda"

# Check number of hauls per species
hlcod0 %>% distinct(haul.id) %>% nrow()
hlfle0 %>% distinct(haul.id) %>% nrow()
hlpla0 %>% distinct(haul.id) %>% nrow()
hldab0 %>% distinct(haul.id) %>% nrow()
```

### Create (unstandardized) CPUE for `SpecVal=1`. If `DataType=C` then `CPUEun=HLNoAtLngt`, if `DataType=R` then `CPUEun=HLNoAtLngt/(HaulDur/60)`, if `DataType=S` then `CPUEun=(HLNoAtLngt*SubFactor)/(HaulDur/60)`. If `SpecVal="zeroCatch"` then `CPUEun=0`, if `SpecVal=4` we need to decide (no length measurements, only total catch). Note that here we also add zero CPUE if `SpecVal=="zeroCatch"`.

Then I will sum for the same haul the CPUE of the same length classes if they were sampled with different subfactors or with different sexes.

```{r, message=FALSE}
# Cod
hlcod0 <- hlcod0 %>%
  mutate(CPUEun = ifelse(SpecVal == "1" & DataType == "C",
                         HLNoAtLngt,
                         
                         ifelse(SpecVal == "1" & DataType == "R",
                                HLNoAtLngt/(HaulDur/60),
                                
                                ifelse(SpecVal == "1" & DataType == "S",
                                       (HLNoAtLngt*SubFactor)/(HaulDur/60),
                                       
                                       ifelse(SpecVal == "zeroCatch", 0, NA)))))

# Plot and fill by zero catch
hlcod0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  filter(!CPUEun_haul == 0)

hlcod0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>%
  group_by(Year, zero_catch) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")

# Some rows have multiple rows per combination of length class and haul id (i suppose often because it's split by sex), so we need to sum it up 
hlcod0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% distinct(n)
hlcod0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% filter(n == 2) %>% as.data.frame() %>% head(20)
test <- hlcod0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% filter(n == 2)
test_id <- test$haul.id[2]

hlcodL <- hlcod0 %>% 
  group_by(LngtClass, haul.id) %>% 
  mutate(CPUEun = sum(CPUEun)) %>%
  ungroup() %>% 
  mutate(id3 = paste(haul.id, LngtClass)) %>% 
  distinct(id3, .keep_all = TRUE) %>% 
  dplyr::select(-X, -id3) # Clean up a bit

# Check with an ID
filter(hlcod0, haul.id == test_id)
filter(hlcodL, haul.id == test_id) %>% as.data.frame()

# Do we still have 0 catches?
hlcodL %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  filter(!CPUEun_haul == 0)

hlcodL %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>%
  group_by(Year, zero_catch) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")


# Flounder
hlfle0 <- hlfle0 %>%
  mutate(CPUEun = ifelse(SpecVal == "1" & DataType == "C",
                         HLNoAtLngt,
                         
                         ifelse(SpecVal == "1" & DataType == "R",
                                HLNoAtLngt/(HaulDur/60),
                                
                                ifelse(SpecVal == "1" & DataType == "S",
                                       (HLNoAtLngt*SubFactor)/(HaulDur/60),
                                       
                                       ifelse(SpecVal == "zeroCatch", 0, NA)))))

# Plot and fill by zero catch
hlfle0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  filter(!CPUEun_haul == 0)

hlfle0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>%
  group_by(Year, zero_catch) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")

# Some rows have multiple rows per combination of length class and haul id (i suppose often because it's split by sex), so we need to sum it up 
hlfle0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% distinct(n)
hlfle0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% filter(n == 2) %>% as.data.frame() %>% head(20)
test <- hlfle0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% filter(n == 2)
test_id <- test$haul.id[2]

hlfleL <- hlfle0 %>% 
  group_by(LngtClass, haul.id) %>% 
  mutate(CPUEun = sum(CPUEun)) %>%
  ungroup() %>% 
  mutate(id3 = paste(haul.id, LngtClass)) %>% 
  distinct(id3, .keep_all = TRUE) %>% 
  dplyr::select(-X, -id3) # Clean up a bit

# Check with an ID
filter(hlfle0, haul.id == test_id)
filter(hlfleL, haul.id == test_id) %>% as.data.frame()

# Do we still have 0 catches?
hlfleL %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  filter(!CPUEun_haul == 0)

hlfleL %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>%
  group_by(Year, zero_catch) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")


# Plaice
hlpla0 <- hlpla0 %>%
  mutate(CPUEun = ifelse(SpecVal == "1" & DataType == "C",
                         HLNoAtLngt,
                         
                         ifelse(SpecVal == "1" & DataType == "R",
                                HLNoAtLngt/(HaulDur/60),
                                
                                ifelse(SpecVal == "1" & DataType == "S",
                                       (HLNoAtLngt*SubFactor)/(HaulDur/60),
                                       
                                       ifelse(SpecVal == "zeroCatch", 0, NA)))))

# Plot and fill by zero catch
hlpla0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  filter(!CPUEun_haul == 0)

hlpla0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>%
  group_by(Year, zero_catch) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")

# Some rows have multiple rows per combination of length class and haul id (i suppose often because it's split by sex), so we need to sum it up 
hlpla0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% distinct(n)
hlpla0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% filter(n == 2) %>% as.data.frame() %>% head(20)
test <- hlpla0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% filter(n == 2)
test_id <- test$haul.id[2]

hlplaL <- hlpla0 %>% 
  group_by(LngtClass, haul.id) %>% 
  mutate(CPUEun = sum(CPUEun)) %>%
  ungroup() %>% 
  mutate(id3 = paste(haul.id, LngtClass)) %>% 
  distinct(id3, .keep_all = TRUE) %>% 
  dplyr::select(-X, -id3) # Clean up a bit

# Check with an ID
filter(hlpla0, haul.id == test_id)
filter(hlplaL, haul.id == test_id) %>% as.data.frame()

# Do we still have 0 catches?
hlplaL %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  filter(!CPUEun_haul == 0)

hlplaL %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>%
  group_by(Year, zero_catch) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")


# Dab
hldab0 <- hldab0 %>%
  mutate(CPUEun = ifelse(SpecVal == "1" & DataType == "C",
                         HLNoAtLngt,
                         
                         ifelse(SpecVal == "1" & DataType == "R",
                                HLNoAtLngt/(HaulDur/60),
                                
                                ifelse(SpecVal == "1" & DataType == "S",
                                       (HLNoAtLngt*SubFactor)/(HaulDur/60),
                                       
                                       ifelse(SpecVal == "zeroCatch", 0, NA)))))

# Plot and fill by zero catch
hldab0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  filter(!CPUEun_haul == 0)

hldab0 %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>%
  group_by(Year, zero_catch) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")

# Some rows have multiple rows per combination of length class and haul id (i suppose often because it's split by sex), so we need to sum it up 
hldab0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% distinct(n)
hldab0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% filter(n == 2) %>% as.data.frame() %>% head(20)
test <- hldab0 %>% group_by(LngtClass, haul.id) %>% mutate(n = n()) %>% ungroup() %>% filter(n == 2)
test_id <- test$haul.id[2]

hldabL <- hldab0 %>% 
  group_by(LngtClass, haul.id) %>% 
  mutate(CPUEun = sum(CPUEun)) %>%
  ungroup() %>% 
  mutate(id3 = paste(haul.id, LngtClass)) %>% 
  distinct(id3, .keep_all = TRUE) %>% 
  dplyr::select(-X, -id3) # Clean up a bit

# Check with an ID
filter(hldab0, haul.id == test_id)
filter(hldabL, haul.id == test_id) %>% as.data.frame()

# Do we still have 0 catches?
hldabL %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  filter(!CPUEun_haul == 0)

hldabL %>%
  group_by(haul.id, Year) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>%
  group_by(Year, zero_catch) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(x = Year, y = n, fill = zero_catch)) +
  geom_bar(stat = "identity")
```

### Get and add annual weight-length relationships from the CA data so that I can calculate CPUE in biomass rather than numbers further down

```{r, message=FALSE}
# Cod
bits_ca_cod <- bits_ca %>% 
  filter(SpecCode %in% c("164712", "126436")) %>% 
  mutate(StNo = as.numeric(StNo)) %>% 
  mutate(Species = "Cod") %>% 
  mutate(ID = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

# Now I need to copy rows with NoAtLngt > 1 so that 1 row = 1 ind
# First make a small test
# nrow(bits_ca_cod)
# test_id <- head(filter(bits_ca_cod, CANoAtLngt == 5))$ID[1]
# filter(bits_ca_cod, ID == test_id & CANoAtLngt == 5)

bits_ca_cod <- bits_ca_cod %>% map_df(., rep, .$CANoAtLngt)

# head(data.frame(filter(bits_ca_cod, ID == test_id & CANoAtLngt == 5)), 20)
# nrow(bits_ca_cod)
# Looks ok!

# Standardize length and drop NA weights (need that for condition)
bits_ca_cod <- bits_ca_cod %>% 
  drop_na(IndWgt) %>% 
  drop_na(LngtClass) %>% 
  filter(IndWgt > 0 & LngtClass > 0) %>%  # Filter positive length and weight
  mutate(weight_kg = IndWgt/1000) %>% 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) # Standardize length ((https://vocab.ices.dk/?ref=18))

# Plot
ggplot(bits_ca_cod, aes(IndWgt, length_cm)) +
  geom_point() + 
  facet_wrap(~Year)

# Now extract the coefficients for each year (not bothering with outliers at the moment)
cod_intercept <- bits_ca_cod %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "(Intercept)") %>% 
  mutate(a = exp(estimate)) %>% 
  mutate(Year = as.integer(Year)) %>% 
  dplyr::select(Year, a)

cod_slope <- bits_ca_cod %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "log(length_cm)") %>% 
  mutate(Year = as.integer(Year)) %>% 
  rename("b" = "estimate") %>% 
  dplyr::select(Year, b)

mean(cod_intercept$a)
mean(cod_slope$b)

# Flounder
bits_ca_fle <- bits_ca %>% 
  filter(SpecCode %in% c("127141", "172894")) %>% 
  mutate(StNo = as.numeric(StNo)) %>% 
  mutate(Species = "Flounder") %>% 
  mutate(ID = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_ca_fle <- bits_ca_fle %>% map_df(., rep, .$CANoAtLngt)

# Standardize length and drop NA weights (need that for condition)
bits_ca_fle <- bits_ca_fle %>% 
  drop_na(IndWgt) %>% 
  drop_na(LngtClass) %>% 
  filter(IndWgt > 0 & LngtClass > 0) %>%  # Filter positive length and weight
  mutate(weight_kg = IndWgt/1000) %>% 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) %>% # Standardize length ((https://vocab.ices.dk/?ref=18))
  mutate(keep = ifelse(LngtCode == "." & Year == 2008, "N", "Y")) %>%
  filter(keep == "Y") %>% 
  filter(length_cm < 70)

# Plot
ggplot(bits_ca_fle, aes(IndWgt, length_cm, color = LngtCode)) +
  geom_point() + 
  facet_wrap(~Year)

# Now extract the coefficients for each year (not bothering with outliers at the moment)
fle_intercept <- bits_ca_fle %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "(Intercept)") %>% 
  mutate(a = exp(estimate)) %>% 
  mutate(Year = as.integer(Year)) %>% 
  dplyr::select(Year, a)

fle_slope <- bits_ca_fle %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "log(length_cm)") %>% 
  mutate(Year = as.integer(Year)) %>% 
  rename("b" = "estimate") %>% 
  dplyr::select(Year, b)

mean(fle_intercept$a)
mean(fle_slope$b)


# Plaice
bits_ca_pla <- bits_ca %>% 
  filter(SpecCode %in% c("127143", "172902")) %>% 
  mutate(StNo = as.numeric(StNo)) %>% 
  mutate(Species = "Plaice") %>% 
  mutate(ID = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_ca_pla <- bits_ca_pla %>% map_df(., rep, .$CANoAtLngt)

# Standardize length and drop NA weights (need that for condition)
bits_ca_pla <- bits_ca_pla %>% 
  drop_na(IndWgt) %>% 
  drop_na(LngtClass) %>% 
  filter(IndWgt > 0 & LngtClass > 0) %>%  # Filter positive length and weight
  mutate(weight_kg = IndWgt/1000) %>% 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) %>% # Standardize length ((https://vocab.ices.dk/?ref=18))
  mutate(keep = ifelse(LngtCode == "." & Year == 2008, "N", "Y")) %>%
  filter(keep == "Y") %>% 
  filter(length_cm < 70)

# Plot
ggplot(bits_ca_pla, aes(IndWgt, length_cm, color = LngtCode)) +
  geom_point() + 
  facet_wrap(~Year)

# Now extract the coefficients for each year (not bothering with outliers at the moment)
pla_intercept <- bits_ca_pla %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "(Intercept)") %>% 
  mutate(a = exp(estimate)) %>% 
  mutate(Year = as.integer(Year)) %>% 
  dplyr::select(Year, a)

pla_slope <- bits_ca_pla %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "log(length_cm)") %>% 
  mutate(Year = as.integer(Year)) %>% 
  rename("b" = "estimate") %>% 
  dplyr::select(Year, b)

mean(pla_intercept$a)
mean(pla_slope$b)


# Dab
bits_ca_dab <- bits_ca %>% 
  filter(SpecCode %in% c("127139", "172881")) %>% 
  mutate(StNo = as.numeric(StNo)) %>% 
  mutate(Species = "Dab") %>% 
  mutate(ID = paste(Year, Quarter, Country, Ship, Gear, StNo, HaulNo, sep = "."))

bits_ca_dab <- bits_ca_dab %>% map_df(., rep, .$CANoAtLngt)

# Standardize length and drop NA weights (need that for condition)
bits_ca_dab <- bits_ca_dab %>% 
  drop_na(IndWgt) %>% 
  drop_na(LngtClass) %>% 
  filter(IndWgt > 0 & LngtClass > 0) %>%  # Filter positive length and weight
  mutate(weight_kg = IndWgt/1000) %>% 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) %>% # Standardize length ((https://vocab.ices.dk/?ref=18))
  mutate(keep = ifelse(LngtCode == "." & Year == 2008, "N", "Y")) %>%
  filter(keep == "Y") %>% 
  filter(length_cm < 70)

# Plot
ggplot(bits_ca_dab, aes(IndWgt, length_cm, color = LngtCode)) +
  geom_point() + 
  facet_wrap(~Year)

bits_ca_dab <- bits_ca_dab %>% filter(IndWgt < 1000)

# Now extract the coefficients for each year (not bothering with outliers at the moment)
dab_intercept <- bits_ca_dab %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "(Intercept)") %>% 
  mutate(a = exp(estimate)) %>% 
  mutate(Year = as.integer(Year)) %>% 
  dplyr::select(Year, a)

dab_slope <- bits_ca_dab %>%
  split(.$Year) %>%
  purrr::map(~lm(log(IndWgt) ~ log(length_cm), data = .x)) %>%
  purrr::map_df(broom::tidy, .id = 'Year') %>%
  filter(term == "log(length_cm)") %>% 
  mutate(Year = as.integer(Year)) %>% 
  rename("b" = "estimate") %>% 
  dplyr::select(Year, b)

mean(dab_intercept$a)
mean(dab_slope$b)
```

### Join the annual L-W relationships to the respective catch data to calculate CPUE in biomass not abundance

```{r, message=FALSE}
# These are the haul-data
# hlcodL
# hlfleL
# hlplaL
# hldabL

hlcodL <- left_join(hlcodL, cod_intercept, by = "Year")
hlcodL <- left_join(hlcodL, cod_slope, by = "Year")

hlfleL <- left_join(hlfleL, fle_intercept, by = "Year")
hlfleL <- left_join(hlfleL, fle_slope, by = "Year")

hlplaL <- left_join(hlplaL, pla_intercept, by = "Year")
hlplaL <- left_join(hlplaL, pla_slope, by = "Year")

# Now replace NA a and b (don't have individual data for all years) with the mean
hlplaL <- hlplaL %>% 
  mutate(a = ifelse(is.na(a), mean(a, na.rm = TRUE), a),
         b = ifelse(is.na(b), mean(b, na.rm = TRUE), b))

hldabL <- left_join(hldabL, dab_intercept, by = "Year")
hldabL <- left_join(hldabL, dab_slope, by = "Year")

# Now replace NA a and b (don't have individual data for all years) with the mean
hldabL <- hldabL %>% 
  mutate(a = ifelse(is.na(a), mean(a, na.rm = TRUE), a),
         b = ifelse(is.na(b), mean(b, na.rm = TRUE), b))
```

### Convert from CPUE in numbers to kg

```{r, message=FALSE}
# Cod
# First standardize length to cm and then check how zero-catches are implemented at this stage
hlcodL <- hlcodL %>% 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) # Standardize length ((https://vocab.ices.dk/?ref=18))

filter(hlcodL, length_cm == 0) # No such thing

# Now check if all rows where length is NA are the ones with zero catch!
hlcodL %>% 
  mutate(length2 = replace_na(length_cm, -9),
         no_length = ifelse(length2 < 0, "T", "F")) %>% 
  ggplot(., aes(length2, CPUEun, color = no_length)) + geom_point(alpha = 0.2) + facet_wrap(~no_length)

hlcodL %>% filter(CPUEun == 0) %>% distinct(length_cm)

# Right, so all hauls with zero catch have NA length_cm. I don't have any NA catches
t <- hlcodL %>% drop_na(CPUEun)
t <- hlcodL %>% filter(CPUEun == 0)
t <- hlcodL %>% drop_na(length_cm)

# In other words, a zero catch is when the catch is zero and length_cm is NA
# In order to not get any NA CPUEs in unit biomass because length is NA (I want them instead
# to be 0, as the numbers-CPUE is), I will replace length_cm == NA with length_cm == 0 before
# calculating biomass CPUE
hlcodL <- hlcodL %>% mutate(length_cm2 = replace_na(length_cm, 0))

# Standardize length in the haul-data and calculate weight
hlcodL <- hlcodL %>% 
  mutate(weight_kg = (a*length_cm2^b)/1000) %>% 
  mutate(CPUEun_kg = weight_kg*CPUEun)

# Plot and check it's correct also in this data
ggplot(hlcodL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)

# Hmm, some unrealistic weights actually
hlcodL %>% arrange(desc(weight_kg)) %>% as.data.frame() %>% head(50)
hlcodL <- hlcodL %>% filter(weight_kg < 100 & length_cm2 < 135)

ggplot(hlcodL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)


# Flounder
# First standardize length to cm and then check how zero-catches are implemented at this stage
hlfleL <- hlfleL %>% 
  mutate(length_cm = ifelse(LngtCode %in% c(".", "0"), 
                            LngtClass/10,
                            LngtClass)) # Standardize length (https://vocab.ices.dk/?ref=18)

filter(hlfleL, length_cm == 0) # No such thing

bits_ca_fle <- bits_ca_fle %>% 
  drop_na(IndWgt) %>% 
  drop_na(LngtClass) %>% 
  filter(IndWgt > 0 & LngtClass > 0) %>%  # Filter positive length and weight
  mutate(weight_kg = IndWgt/1000) %>% 
  mutate(length_cm = ifelse(LngtCode == ".", 
                            LngtClass/10,
                            LngtClass)) %>% # Standardize length ((https://vocab.ices.dk/?ref=18))
  mutate(keep = ifelse(LngtCode == "." & Year == 2008, "N", "Y")) %>%
  filter(keep == "Y") %>% 
  filter(length_cm < 70)

# Now check if all rows where length is NA are the ones with zero catch!
hlfleL %>% 
  mutate(length2 = replace_na(length_cm, -9),
         no_length = ifelse(length2 < 0, "T", "F")) %>% 
  ggplot(., aes(length2, CPUEun, color = no_length)) + geom_point(alpha = 0.2) + facet_wrap(~no_length)

hlfleL %>% mutate(length2 = replace_na(length_cm, -9)) %>% group_by(length2) %>% distinct(CPUEun) %>% arrange(CPUEun)

# Right, so all hauls with zero catch have NA length_cm. I don't have any NA catches
t <- hlfleL %>% drop_na(CPUEun)
# Well, 11 rows. I will remove them
hlfleL <- hlfleL %>% drop_na(CPUEun)
t <- hlfleL %>% filter(CPUEun == 0) %>% distinct(length_cm)
t <- hlfleL %>% drop_na(length_cm)

# In other words, a zero catch is when the catch is zero and length_cm is NA
# In order to not get any NA CPUEs in unit biomass because length is NA (I want them instead
# to be 0, as the numbers-CPUE is), I will replace length_cm == NA with length_cm == 0 before
# calculating biomass cpue
hlfleL <- hlfleL %>% mutate(length_cm2 = replace_na(length_cm, 0))

# Standardize length in the haul-data and calculate weight
hlfleL <- hlfleL %>% 
  mutate(weight_kg = (a*length_cm2^b)/1000) %>% 
  mutate(CPUEun_kg = weight_kg*CPUEun)

# Plot and check it's correct also in this data
ggplot(hlfleL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)

# Check
t <- hlfleL %>% drop_na(CPUEun_kg) # Should not have any NA in biomass-catch
t <- hlfleL %>% filter(CPUEun_kg == 0) # Should result in a few percent of rows (note this is not proportion of hauls, but rows)
t <- hlfleL %>% drop_na(length_cm2) # Should be no NA


# Plaice
# First standardize length to cm and then check how zero-catches are implemented at this stage
hlplaL <- hlplaL %>% 
  mutate(length_cm = ifelse(LngtCode %in% c(".", "0"), 
                            LngtClass/10,
                            LngtClass)) # Standardize length (https://vocab.ices.dk/?ref=18)

filter(hlplaL, length_cm == 0) # No such thing

# Now check if all rows where length is NA are the ones with zero catch!
hlplaL %>% 
  mutate(length2 = replace_na(length_cm, -9),
         no_length = ifelse(length2 < 0, "T", "F")) %>% 
  ggplot(., aes(length2, CPUEun, color = no_length)) + geom_point(alpha = 0.2) + facet_wrap(~no_length)

hlplaL %>% mutate(length2 = replace_na(length_cm, -9)) %>% group_by(length2) %>% distinct(CPUEun) %>% arrange(CPUEun)

# Right, so all hauls with zero catch have NA length_cm. I don't have any NA catches (but I do)
t <- hlplaL %>% drop_na(CPUEun)
hlplaL <- hlplaL %>% drop_na(CPUEun)

# In other words, a zero catch is when the catch is zero and length_cm is NA
# In order to not get any NA CPUEs in unit biomass because length is NA (I want them instead
# to be 0, as the numbers-CPUE is), I will replace length_cm == NA with length_cm == 0 before
# calculating biomass cpue
hlplaL <- hlplaL %>% mutate(length_cm2 = replace_na(length_cm, 0))

# Standardize length in the haul-data and calculate weight
hlplaL <- hlplaL %>% 
  mutate(weight_kg = (a*length_cm2^b)/1000) %>% 
  mutate(CPUEun_kg = weight_kg*CPUEun)

# Plot and check it's correct also in this data
ggplot(hlplaL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)

# Check
t <- hlfleL %>% drop_na(CPUEun_kg) # Should not have any NA in biomass-catch
t <- hlfleL %>% filter(CPUEun_kg == 0) # Should result in a few percent of rows (note this is not proportion of hauls, but rows)
t <- hlfleL %>% drop_na(length_cm2) # Should be no NA


# Dab
# First standardize length to cm and then check how zero-catches are implemented at this stage
hldabL <- hldabL %>% 
  mutate(length_cm = ifelse(LngtCode %in% c(".", "0"), 
                            LngtClass/10,
                            LngtClass)) # Standardize length (https://vocab.ices.dk/?ref=18)

filter(hldabL, length_cm == 0) # No such thing

# Now check if all rows where length is NA are the ones with zero catch!
hldabL %>% 
  mutate(length2 = replace_na(length_cm, -9),
         no_length = ifelse(length2 < 0, "T", "F")) %>% 
  ggplot(., aes(length2, CPUEun, color = no_length)) + geom_point(alpha = 0.2) + facet_wrap(~no_length)

hldabL %>% mutate(length2 = replace_na(length_cm, -9)) %>% group_by(length2) %>% distinct(CPUEun) %>% arrange(CPUEun)

# Right, so all hauls with zero catch have NA length_cm. I don't have any NA catches
t <- hldabL %>% drop_na(CPUEun)

# In other words, a zero catch is when the catch is zero and length_cm is NA
# In order to not get any NA CPUEs in unit biomass because length is NA (I want them instead
# to be 0, as the numbers-CPUE is), I will replace length_cm == NA with length_cm == 0 before
# calculating biomass cpue
hldabL <- hldabL %>% mutate(length_cm2 = replace_na(length_cm, 0))

# Standardize length in the haul-data and calculate weight
hldabL <- hldabL %>% 
  mutate(weight_kg = (a*length_cm2^b)/1000) %>% 
  mutate(CPUEun_kg = weight_kg*CPUEun)

# Plot and check it's correct also in this data
ggplot(hldabL, aes(weight_kg, length_cm2)) +
  geom_point() + 
  facet_wrap(~Year)

# Check
t <- hldabL %>% drop_na(CPUEun_kg) # Should not have any NA in biomass-catch
t <- hldabL %>% filter(CPUEun_kg == 0) # Should result in a few percent of rows (note this is not proportion of hauls, but rows)
t <- hldabL %>% drop_na(length_cm2) # Should be no NA
```

```{r proportion of zero-catch hauls}
cod_0plot <- hlcodL %>%
  group_by(haul.id, Year, Quarter) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>% 
  group_by(Year, Quarter, zero_catch) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from = zero_catch, values_from = n) %>% 
  mutate(prop_zero_catch_hauls = Y/(N+Y),
         species = "Cod") 

fle_0plot <- hlfleL %>%
  group_by(haul.id, Year, Quarter) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>% 
  group_by(Year, Quarter, zero_catch) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from = zero_catch, values_from = n) %>% 
  mutate(prop_zero_catch_hauls = Y/(N+Y),
         species = "Flounder")

pla_0plot <- hlplaL %>%
  group_by(haul.id, Year, Quarter) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>% 
  group_by(Year, Quarter, zero_catch) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from = zero_catch, values_from = n) %>% 
  mutate(prop_zero_catch_hauls = Y/(N+Y),
         species = "Plaice")

dab_0plot <- hldabL %>%
  group_by(haul.id, Year, Quarter) %>%
  summarise(CPUEun_haul = sum(CPUEun)) %>% 
  ungroup() %>% 
  mutate(zero_catch = ifelse(CPUEun_haul == 0, "Y", "N")) %>% 
  group_by(Year, Quarter, zero_catch) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from = zero_catch, values_from = n) %>% 
  mutate(prop_zero_catch_hauls = Y/(N+Y),
         species = "Dab")
```

### Standardize according to Orio
To get unit: kg of fish caught by trawling for 1 h a standard bottom swept area of 0.45km2 using a TVL trawl with 75 m sweeps at the standard speed of three knots

```{r, message=FALSE}
# Remove hauls done with the TVL gear with a SweepLngt < 50 (these are calibration hauls, pers. com. Anders & Ale)
# And also hauls without length-information
# Remove pelagic gear
hlcodL <- hlcodL %>%
  mutate(SweepLngt2 = replace_na(SweepLngt, 50)) %>% 
  mutate(keep = ifelse(Gear == "TVL" & SweepLngt2 < 50, "N", "Y")) %>% 
  filter(keep == "Y") %>% 
  dplyr::select(-keep, -SweepLngt2) %>% 
  filter(!Gear == "PEL")
  
hlfleL <- hlfleL %>%
  mutate(SweepLngt2 = replace_na(SweepLngt, 50)) %>% 
  mutate(keep = ifelse(Gear == "TVL" & SweepLngt2 < 50, "N", "Y")) %>% 
  filter(keep == "Y") %>% 
  dplyr::select(-keep, -SweepLngt2) %>% 
  filter(!Gear == "PEL")

hlplaL <- hlplaL %>%
  mutate(SweepLngt2 = replace_na(SweepLngt, 50)) %>% 
  mutate(keep = ifelse(Gear == "TVL" & SweepLngt2 < 50, "N", "Y")) %>% 
  filter(keep == "Y") %>% 
  dplyr::select(-keep, -SweepLngt2) %>% 
  filter(!Gear == "PEL")

hldabL <- hldabL %>%
  mutate(SweepLngt2 = replace_na(SweepLngt, 50)) %>% 
  mutate(keep = ifelse(Gear == "TVL" & SweepLngt2 < 50, "N", "Y")) %>% 
  filter(keep == "Y") %>% 
  dplyr::select(-keep, -SweepLngt2) %>% 
  filter(!Gear == "PEL")

# Add in RS and RSA-values from the sweep file
# CPUE should be multiplied with RS and RSA to standardize to a relative speed and gear dimension.
# There is not a single file will all RS and RSA values. Instead they come in three files:
# - sweep (non-Swedish hauls between 1991-2016)
# - + calculated based on trawl speed and gear dimensions.
# I will join in the RS and RSA values from all sources, then standardize and filter
# away non-standardized hauls
# sort(unique(sweep$Year))
# sort(unique(sweep$Country))

# Since I don't have the sweep data for Swedish data, I have to calculate it from scratch using the 
# equation in Orio's spreadsheet

# First I will join in the sweep data, 
sweep_sel <- sweep %>% rename("haul.id" = "ï..haul.id") %>% dplyr::select(haul.id, RSA, RS)

hlcodL2 <- left_join(hlcodL, sweep_sel)
hlfleL2 <- left_join(hlfleL, sweep_sel)
hlplaL2 <- left_join(hlplaL, sweep_sel)
hldabL2 <- left_join(hldabL, sweep_sel)

hlcodL2 <- hlcodL2 %>%
  rename("RS_sweep" = "RS",
         "RSA_sweep" = "RSA") %>% 
  mutate(RS_sweep = as.numeric(RS_sweep),
         RSA_sweep = as.numeric(RSA_sweep))

hlfleL2 <- hlfleL2 %>%
  rename("RS_sweep" = "RS",
         "RSA_sweep" = "RSA") %>% 
  mutate(RS_sweep = as.numeric(RS_sweep),
         RSA_sweep = as.numeric(RSA_sweep))

hlplaL2 <- hlplaL2 %>%
  rename("RS_sweep" = "RS",
         "RSA_sweep" = "RSA") %>% 
  mutate(RS_sweep = as.numeric(RS_sweep),
         RSA_sweep = as.numeric(RSA_sweep))

hldabL2 <- hldabL2 %>%
  rename("RS_sweep" = "RS",
         "RSA_sweep" = "RSA") %>% 
  mutate(RS_sweep = as.numeric(RS_sweep),
         RSA_sweep = as.numeric(RSA_sweep))


# I will calculate a RS and RSA column in the catch data based on Ale's equation in the sweep file:
sort(unique(hlcodL2$GroundSpeed))
sort(unique(hlcodL2$Fishing.line))
sort(unique(hlcodL2$SweepLngt))

# First replace -9 in the columns I use for the calculations with NA so I don't end up with real numbers that are wrong!
hlcodL2 <- hlcodL2 %>% mutate(GroundSpeed = ifelse(GroundSpeed == -9, NA, GroundSpeed),
                              Fishing.line = ifelse(Fishing.line == -9, NA, Fishing.line),
                              SweepLngt = ifelse(SweepLngt == -9, NA, SweepLngt))

hlfleL2 <- hlfleL2 %>% mutate(GroundSpeed = ifelse(GroundSpeed == -9, NA, GroundSpeed),
                              Fishing.line = ifelse(Fishing.line == -9, NA, Fishing.line),
                              SweepLngt = ifelse(SweepLngt == -9, NA, SweepLngt))

hlplaL2 <- hlplaL2 %>% mutate(GroundSpeed = ifelse(GroundSpeed == -9, NA, GroundSpeed),
                              Fishing.line = ifelse(Fishing.line == -9, NA, Fishing.line),
                              SweepLngt = ifelse(SweepLngt == -9, NA, SweepLngt))

hldabL2 <- hldabL2 %>% mutate(GroundSpeed = ifelse(GroundSpeed == -9, NA, GroundSpeed),
                              Fishing.line = ifelse(Fishing.line == -9, NA, Fishing.line),
                              SweepLngt = ifelse(SweepLngt == -9, NA, SweepLngt))


hlcodL2 %>% filter(Quarter == 1) %>%
  distinct(GroundSpeed, Fishing.line, SweepLngt) %>% as.data.frame() %>% head(50)

hlcodL2 %>% filter(Quarter == 4) %>%
  distinct(GroundSpeed, Fishing.line, SweepLngt) %>% as.data.frame() %>% head(50)

# Hmm, Q1 has at least one of the RS or RSA variables as NAs. Will be difficult to standardize!
# Hope the correction factors are present in Ales conversion data

# Now calculate correction factors
hlcodL2 <- hlcodL2 %>% mutate(RS_x = 3/GroundSpeed,
                              Horizontal.opening..m. = Fishing.line*0.67,
                              Swep.one.side..after.formula...meter = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                              Size.final..m = Horizontal.opening..m. + (Swep.one.side..after.formula...meter*2),
                              Swept.area = (Size.final..m*3*1860)/1000000,
                              RSA_x = 0.45388309675081/Swept.area)

hlfleL2 <- hlfleL2 %>% mutate(RS_x = 3/GroundSpeed,
                              Horizontal.opening..m. = Fishing.line*0.67,
                              Swep.one.side..after.formula...meter = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                              Size.final..m = Horizontal.opening..m. + (Swep.one.side..after.formula...meter*2),
                              Swept.area = (Size.final..m*3*1860)/1000000,
                              RSA_x = 0.45388309675081/Swept.area)

hlplaL2 <- hlplaL2 %>% mutate(RS_x = 3/GroundSpeed,
                              Horizontal.opening..m. = Fishing.line*0.67,
                              Swep.one.side..after.formula...meter = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                              Size.final..m = Horizontal.opening..m. + (Swep.one.side..after.formula...meter*2),
                              Swept.area = (Size.final..m*3*1860)/1000000,
                              RSA_x = 0.45388309675081/Swept.area)

hldabL2 <- hldabL2 %>% mutate(RS_x = 3/GroundSpeed,
                              Horizontal.opening..m. = Fishing.line*0.67,
                              Swep.one.side..after.formula...meter = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                              Size.final..m = Horizontal.opening..m. + (Swep.one.side..after.formula...meter*2),
                              Swept.area = (Size.final..m*3*1860)/1000000,
                              RSA_x = 0.45388309675081/Swept.area)

# Check EQ. is correct by recalculating it in the sweep file
sweep <- sweep %>% mutate(Horizontal.opening..m.2 = Fishing.line*0.67,
                          Swep.one.side..after.formula...meter2 = 0.258819045*SweepLngt, # SIN(RADIANS(15))
                          Size.final..m2 = Horizontal.opening..m.2 + (Swep.one.side..after.formula...meter2*2),
                          Swept.area2 = (Size.final..m2*3*1860)/1000000,
                          RSA_x = 0.45388309675081/Swept.area2)

sweep %>%
  drop_na() %>%
  ggplot(., aes(as.numeric(RSA), RSA_x)) + geom_point() + geom_abline(intercept = 0, slope = 1)
# Yes it's the same

# Replace NAs with -1/3 (because ICES codes missing values as -9 and in the calculation above they get -1/3),
# so that I can filter them easily later
# sort(unique(hlcodL2$RS_x))
# sort(unique(hlcodL2$RSA_x))

hlcodL2$RS_x[is.na(hlcodL2$RS_x)] <- -1/3
hlcodL2$RS_sweep[is.na(hlcodL2$RS_sweep)] <- -1/3
hlcodL2$RSA_x[is.na(hlcodL2$RSA_x)] <- -1/3
hlcodL2$RSA_sweep[is.na(hlcodL2$RSA_sweep)] <- -1/3

hlfleL2$RS_x[is.na(hlfleL2$RS_x)] <- -1/3
hlfleL2$RS_sweep[is.na(hlfleL2$RS_sweep)] <- -1/3
hlfleL2$RSA_x[is.na(hlfleL2$RSA_x)] <- -1/3
hlfleL2$RSA_sweep[is.na(hlfleL2$RSA_sweep)] <- -1/3

hlplaL2$RS_x[is.na(hlplaL2$RS_x)] <- -1/3
hlplaL2$RS_sweep[is.na(hlplaL2$RS_sweep)] <- -1/3
hlplaL2$RSA_x[is.na(hlplaL2$RSA_x)] <- -1/3
hlplaL2$RSA_sweep[is.na(hlplaL2$RSA_sweep)] <- -1/3

hldabL2$RS_x[is.na(hldabL2$RS_x)] <- -1/3
hldabL2$RS_sweep[is.na(hldabL2$RS_sweep)] <- -1/3
hldabL2$RSA_x[is.na(hldabL2$RSA_x)] <- -1/3
hldabL2$RSA_sweep[is.na(hldabL2$RSA_sweep)] <- -1/3

# Compare the difference correction factors (calculated vs imported from sweep file)
p1 <- ggplot(filter(hlcodL2, RS_x > 0), aes(RS_x)) + geom_histogram() + xlim(0.4, 1.76)
p2 <- ggplot(hlcodL2, aes(RSA_x)) + geom_histogram()
p3 <- ggplot(hlcodL2, aes(RS_sweep)) + geom_histogram()
p4 <- ggplot(hlcodL2, aes(RSA_sweep)) + geom_histogram()

(p1 + p2) / (p3 + p4)

p5 <- ggplot(filter(hlfleL2, RS_x > 0), aes(RS_x)) + geom_histogram() + xlim(0.4, 1.76)
p6 <- ggplot(hlfleL2, aes(RSA_x)) + geom_histogram()
p7 <- ggplot(hlfleL2, aes(RS_sweep)) + geom_histogram()
p8 <- ggplot(hlfleL2, aes(RSA_sweep)) + geom_histogram()

(p5 + p6) / (p7 + p8)

p9 <- ggplot(filter(hlplaL2, RS_x > 0), aes(RS_x)) + geom_histogram() + xlim(0.4, 1.76)
p10 <- ggplot(hlplaL2, aes(RSA_x)) + geom_histogram()
p11 <- ggplot(hlplaL2, aes(RS_sweep)) + geom_histogram()
p12 <- ggplot(hlplaL2, aes(RSA_sweep)) + geom_histogram()

(p9 + p10) / (p11 + p12)

p13 <- ggplot(filter(hldabL2, RS_x > 0), aes(RS_x)) + geom_histogram() + xlim(0.4, 1.76)
p14 <- ggplot(hldabL2, aes(RSA_x)) + geom_histogram()
p15 <- ggplot(hldabL2, aes(RS_sweep)) + geom_histogram()
p16 <- ggplot(hldabL2, aes(RSA_sweep)) + geom_histogram()

(p13 + p14) / (p15 + p16)

# Why do I have RSA values smaller than one? (either because sweep length is longer or gear is larger (GOV))
# Check if I can calculate the same RSA in sweep as that entered there.
# Ok, so the equation is correct. Which ID's have RSA < 1?
hlcodL2 %>% 
  filter(RSA_x < 1 & RSA_x > 0) %>%
  dplyr::select(Year, Country, Ship, Gear, haul.id, Horizontal.opening..m., Fishing.line,
                Swep.one.side..after.formula...meter, SweepLngt, Size.final..m, Swept.area, RSA_x) %>% 
  ggplot(., aes(RSA_x, fill = factor(SweepLngt))) + geom_histogram() + facet_wrap(~Gear, ncol = 1)

# Check if I have more than one unique RS or RSA value per haul, or if it's "either this or that"
# Filter positive in both columns
hlcodL2 %>% filter(RS_x > 0 & RS_sweep > 0) %>% ggplot(., aes(RS_x, RS_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

hlcodL2 %>% filter(RSA_x > 0 & RSA_sweep > 0) %>% ggplot(., aes(RSA_x, RSA_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

hlfleL2 %>% filter(RS_x > 0 & RS_sweep > 0) %>% ggplot(., aes(RS_x, RS_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

hlfleL2 %>% filter(RSA_x > 0 & RSA_sweep > 0) %>% ggplot(., aes(RSA_x, RSA_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

# Ok, there's on odd RS_x that is larger than 3. It didn't catch anything and speed is 0.8! Will remove
hlcodL2 <- hlcodL2 %>% filter(RS_x < 3)
hlfleL2 <- hlfleL2 %>% filter(RS_x < 3)

# Plot again
hlcodL2 %>% filter(RS_x > 0 & RS_sweep > 0) %>% ggplot(., aes(RS_x, RS_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

hlfleL2 %>% filter(RS_x > 0 & RS_sweep > 0) %>% ggplot(., aes(RS_x, RS_sweep)) +
  geom_point() + geom_abline(aes(intercept = 0, slope = 1), color = "red")

# They are largely the same when they overlap. When they differ, I will use RS_sweep
# Make a single RS and RSA column

# Cod 
hlcodL3 <- hlcodL2 %>%
  mutate(RS = -99,
         RS = ifelse(RS_sweep > 0, RS_sweep, RS),
         RS = ifelse(RS < 0 & RS_x > 0, RS_x, RS)) %>% # Note that there are no NA i RS_x. This replaces all non-RS_sweep values -0.3, so I can filter positive later
  mutate(RSA = -99,
         RSA = ifelse(RSA_sweep > 0, RSA_sweep, RSA),
         RSA = ifelse(RSA < 0 & RSA_x > 0, RSA_x, RSA)) %>%
  filter(RS > 0) %>%
  filter(RSA > 0) %>% 
  mutate(RSRSA = RS*RSA)

# Plot
ggplot(hlcodL3, aes(RSRSA)) + geom_histogram()


# Flounder 
hlfleL2 %>% filter(Country == "LAT") %>% distinct(Year) %>% arrange(Year)

hlfleL3 <- hlfleL2 %>%
  mutate(RS = -999,
         RS = ifelse(RS_sweep > 0, RS_sweep, RS),
         RS = ifelse(RS < 0, RS_x, RS)) %>% # Note that there are no NA i RS_x. This replaces all non-RS_sweep values -0.3, so I can filter positive later
  mutate(RSA = -999,
         RSA = ifelse(RSA_sweep > 0, RSA_sweep, RSA),
         RSA = ifelse(RSA < 0, RSA_x, RSA)) %>% 
  filter(RS > 0) %>%
  filter(RSA > 0) %>% 
  mutate(RSRSA = RS*RSA)

# Plot
ggplot(hlfleL3, aes(RSRSA)) + geom_histogram()

# Test how many years of LAT data I miss out on because I can't standardize it.
# hlfleL2 %>%
#   mutate(RS = -999,
#          RS = ifelse(RS_sweep > 0, RS_sweep, RS),
#          RS = ifelse(RS < 0, RS_x, RS)) %>% # Note that there are no NA i RS_x. This replaces all non-RS_sweep values -0.3, so I can filter positive later
#   filter(RS > 0) %>% 
#   filter(Country == "LAT") %>% 
#   distinct(Year) %>% 
#   arrange(Year)
#   
# hlfleL2 %>%
#   mutate(RSA = -999,
#          RSA = ifelse(RSA_sweep > 0, RSA_sweep, RSA),
#          RSA = ifelse(RSA < 0, RSA_x, RSA)) %>% 
#   filter(RSA > 0) %>% 
#   filter(Country == "LAT") %>% 
#   distinct(Year) %>% 
#   arrange(Year)

# Plaice 
hlplaL3 <- hlplaL2 %>%
  mutate(RS = -99,
         RS = ifelse(RS_sweep > 0, RS_sweep, RS),
         RS = ifelse(RS < 0 & RS_x > 0, RS_x, RS)) %>% # Note that there are no NA i RS_x. This replaces all non-RS_sweep values -0.3, so I can filter positive later
  mutate(RSA = -99,
         RSA = ifelse(RSA_sweep > 0, RSA_sweep, RSA),
         RSA = ifelse(RSA < 0 & RSA_x > 0, RSA_x, RSA)) %>%
  filter(RS > 0) %>%
  filter(RSA > 0) %>% 
  mutate(RSRSA = RS*RSA)

# Plot
ggplot(hlplaL3, aes(RSRSA)) + geom_histogram()


# Dab 
hldabL3 <- hldabL2 %>%
  mutate(RS = -99,
         RS = ifelse(RS_sweep > 0, RS_sweep, RS),
         RS = ifelse(RS < 0 & RS_x > 0, RS_x, RS)) %>% # Note that there are no NA i RS_x. This replaces all non-RS_sweep values -0.3, so I can filter positive later
  mutate(RSA = -99,
         RSA = ifelse(RSA_sweep > 0, RSA_sweep, RSA),
         RSA = ifelse(RSA < 0 & RSA_x > 0, RSA_x, RSA)) %>%
  filter(RS > 0) %>%
  filter(RSA > 0) %>% 
  mutate(RSRSA = RS*RSA)

# Plot
ggplot(hldabL3, aes(RSRSA)) + geom_histogram()


# Standardize!
hlcodL3 <- hlcodL3 %>%
  mutate(CPUEst_kg = CPUEun_kg*RS*RSA,
         CPUEst = CPUEun*RS*RSA)

hlfleL3 <- hlfleL3 %>%
  mutate(CPUEst_kg = CPUEun_kg*RS*RSA,
         CPUEst = CPUEun*RS*RSA)

hlplaL3 <- hlplaL3 %>%
  mutate(CPUEst_kg = CPUEun_kg*RS*RSA,
         CPUEst = CPUEun*RS*RSA)
  
hldabL3 <- hldabL3 %>%
  mutate(CPUEst_kg = CPUEun_kg*RS*RSA,
         CPUEst = CPUEun*RS*RSA)


unique(is.na(hlcodL3$CPUEst_kg))
unique(is.na(hlcodL3$CPUEst))
min(hlcodL3$CPUEst_kg)
min(hlcodL3$CPUEst)

unique(is.na(hlfleL3$CPUEst_kg)) # Remove the few NA's here
hlfleL3 <- hlfleL3 %>% drop_na(CPUEst_kg)
unique(is.na(hlfleL3$CPUEst))
min(hlfleL3$CPUEst_kg) 
min(hlfleL3$CPUEst)

unique(is.na(hlplaL3$CPUEst_kg))
unique(is.na(hlplaL3$CPUEst))
min(hlplaL3$CPUEst_kg)
min(hlplaL3$CPUEst)

unique(is.na(hldabL3$CPUEst_kg))
unique(is.na(hldabL3$CPUEst))
min(hldabL3$CPUEst_kg)
min(hldabL3$CPUEst)

# Now calculate CPUE PER LENGTH CLASS, then create the new unit, i.e.: convert from kg of fish caught by trawling for 1 h a standard bottom swept area of 0.45km2 (using a TVL trawl with 75 m sweeps at the standard speed of three knots) to... kg of fish per km^2 by dividing with 0.45

p1 <- ggplot(hlcodL3) +
  geom_histogram(aes(length_cm2, fill = "length_cm1"), alpha = 0.5)  

p2 <- ggplot(hlcodL3) +
  geom_histogram(aes(length_cm2, fill = "length_cm2"), alpha = 0.5) 

p1/p2


# Cod
hlcodhaul <- hlcodL3 %>%
  mutate(cpue_kg = CPUEst_kg,
         cpue = CPUEst,
         cpue_kg_un = CPUEun_kg,
         cpue_un = CPUEun,
         kg = cpue_kg * (HaulDur / 60),
         abund = cpue * (HaulDur / 60),
         density = kg/0.45,
         density_ab = abund/0.45)

# Flounder
hlflehaul <- hlfleL3 %>%
  mutate(cpue_kg = CPUEst_kg,
         cpue = CPUEst,
         cpue_kg_un = CPUEun_kg,
         cpue_un = CPUEun,
         kg = cpue_kg * (HaulDur / 60),
         abund = cpue * (HaulDur / 60),
         density = kg/0.45,
         density_ab = abund/0.45)

# Plaice
hlplahaul <- hlplaL3 %>%
  mutate(cpue_kg = CPUEst_kg,
         cpue = CPUEst,
         cpue_kg_un = CPUEun_kg,
         cpue_un = CPUEun,
         kg = cpue_kg * (HaulDur / 60),
         abund = cpue * (HaulDur / 60),
         density = kg/0.45,
         density_ab = abund/0.45)

# Dab
hldabhaul <- hldabL3 %>%
  mutate(cpue_kg = CPUEst_kg,
         cpue = CPUEst,
         cpue_kg_un = CPUEun_kg,
         cpue_un = CPUEun,
         kg = cpue_kg * (HaulDur / 60),
         abund = cpue * (HaulDur / 60),
         density = kg/0.45,
         density_ab = abund/0.45)

# First, figure out why I have length = 0 and density = 0 when I have other lengths in the haul
hlcodhaul %>% filter(haul.id == "1993:1:GFR:SOL:H20:23:31") %>% as.data.frame()

hlcodhaul %>%
  group_by(haul.id) %>% 
  mutate(no_catches = length(unique(CPUEun))) %>% 
  filter(any(CPUEun == 0)) %>% 
  filter(no_catches > 1) %>% 
  as.data.frame() %>% 
  head(50)

hlcodhaul %>% 
  group_by(haul.id) %>% 
  filter(CPUEun == min(CPUEun)) %>% 
  ungroup() %>% 
  distinct(CPUEun)

# The minimum CPUE in all hauls is always zero at this stage. It doesn't really matter because I calculate haul-level CPUE by grouping by ID's and summing. 

# Rename columns and select specific columns from the cod data
# Cod
datcod <- hlcodhaul %>%
  dplyr::select(density, Year, ShootLat, ShootLong, Quarter, Country, Month, haul.id, IDx, Rect, sub_div, length_cm2) %>% 
  rename(year = Year,
         lat = ShootLat,
         lon = ShootLong,
         quarter = Quarter,
         ices_rect = Rect,
         length_cm = length_cm2) %>% 
  mutate(species = "cod")

# Flounder
datfle <- hlflehaul %>%
  dplyr::select(density, Year, ShootLat, ShootLong, Quarter, Country, Month, haul.id, IDx, Rect, sub_div, length_cm2) %>% 
  rename(year = Year,
         lat = ShootLat,
         lon = ShootLong,
         quarter = Quarter,
         ices_rect = Rect,
         length_cm = length_cm2) %>% 
  mutate(species = "flounder")

# Plaice
datpla <- hlplahaul %>%
  dplyr::select(density, Year, ShootLat, ShootLong, Quarter, Country, Month, haul.id, IDx, Rect, sub_div, length_cm2) %>% 
  rename(year = Year,
         lat = ShootLat,
         lon = ShootLong,
         quarter = Quarter,
         ices_rect = Rect,
         length_cm = length_cm2) %>% 
  mutate(species = "plaice")

# Dab
datdab <- hldabhaul %>%
  dplyr::select(density, Year, ShootLat, ShootLong, Quarter, Country, Month, haul.id, IDx, Rect, sub_div, length_cm2) %>% 
  rename(year = Year,
         lat = ShootLat,
         lon = ShootLong,
         quarter = Quarter,
         ices_rect = Rect,
         length_cm = length_cm2) %>% 
  mutate(species = "dab")
```

```{r very long data with all combinations of size and haul id}
# Because it's size-based cpue, I want the data frame to be "full", so that each haul has every size, even if all are empty. That means I can define a size group, group by and sum for each haul, even if there aren't any sizes caught. Now I only have lengths with catches, and no lengths if catch is zero.
datcod %>% group_by(haul.id) %>% summarise(n_size = length(unique(length_cm))) %>% distinct(n_size, .keep_all = TRUE)
datcod %>% filter(haul.id == "1993:1:GFR:SOL:H20:23:31") %>% as.data.frame()
datcod %>% group_by(haul.id) %>% mutate(tot_dens = sum(density)) %>% ungroup() %>% distinct(haul.id, .keep_all = TRUE) %>% filter(tot_dens == 0)

# Create a data frame with all combinations of trawl IDs and lengths
ex_df <- data.frame(expand.grid(
  length_cm = seq_range(datcod$length_cm, by = 1),
  haul.id = unique(datcod$haul.id))
  )

# Create an ID that is haul + length
ex_df$haul.id.size <- paste(ex_df$haul.id, ex_df$length_cm, sep = ".")
datcod$haul.id.size <- paste(datcod$haul.id, datcod$length_cm, sep = ".")

# Remove IDs that are already in datcod
ex_df <- ex_df %>% filter(!haul.id.size %in% unique(datcod$haul.id.size)) 

# Add in the other columns besides density and length
dat_for_join <- datcod %>% dplyr::select(-density, -length_cm, -haul.id.size) %>% distinct(haul.id, .keep_all = TRUE)

ex_df <- left_join(ex_df, dat_for_join, by = "haul.id")

datcod %>% filter(haul.id.size %in% ex_df$haul.id.size)

# Bind_rows these data with datcod
nrow(datcod) + nrow(ex_df)

unique(is.na(datcod$density))

datcod <- bind_rows(datcod, ex_df) %>% arrange(haul.id, length_cm)
nrow(datcod)
datcod

# Replace NA density with 0 density because that's the added length-classes not previously in the catch data
datcod <- datcod %>% mutate(density = replace_na(density, 0))

# Check the proportion zeroes are still correct:
t <- datcod %>%
  group_by(haul.id) %>%
  summarise(haul_density = sum(density)) %>% 
  ungroup()

nrow(datcod)
length(unique(datcod$haul.id))
t %>% drop_na(haul_density)
nrow(t)
t %>% filter(!haul_density == 0)


# Flounder
# Create a data frame with all combinations of trawl IDs and lengths
ex_df <- data.frame(expand.grid(
  length_cm = seq_range(datfle$length_cm, by = 1),
  haul.id = unique(datfle$haul.id))
  )

# Create an ID that is haul + length
ex_df$haul.id.size <- paste(ex_df$haul.id, ex_df$length_cm, sep = ".")
datfle$haul.id.size <- paste(datfle$haul.id, datfle$length_cm, sep = ".")

# Remove IDs that are already in datfle
ex_df <- ex_df %>% filter(!haul.id.size %in% unique(datfle$haul.id.size)) 

# Add in the other columns besides density and length
dat_for_join <- datfle %>% dplyr::select(-density, -length_cm, -haul.id.size) %>% distinct(haul.id, .keep_all = TRUE)

ex_df <- left_join(ex_df, dat_for_join, by = "haul.id")

# Bind_rows these data with datfle
datfle <- bind_rows(datfle, ex_df) %>% arrange(haul.id, length_cm)

# Replace NA density with 0 density because that's the added length-classes not previously in the catch data
datfle <- datfle %>% mutate(density = replace_na(density, 0))


# Plaice
# Create a data frame with all combinations of trawl IDs and lengths
ex_df <- data.frame(expand.grid(
  length_cm = seq_range(datpla$length_cm, by = 1),
  haul.id = unique(datpla$haul.id))
  )

# Create an ID that is haul + length
ex_df$haul.id.size <- paste(ex_df$haul.id, ex_df$length_cm, sep = ".")
datpla$haul.id.size <- paste(datpla$haul.id, datpla$length_cm, sep = ".")

# Remove IDs that are already in datpla
ex_df <- ex_df %>% filter(!haul.id.size %in% unique(datpla$haul.id.size)) 

# Add in the other columns besides density and length
dat_for_join <- datpla %>% dplyr::select(-density, -length_cm, -haul.id.size) %>% distinct(haul.id, .keep_all = TRUE)

ex_df <- left_join(ex_df, dat_for_join, by = "haul.id")

# Bind_rows these data with datpla
datpla <- bind_rows(datpla, ex_df) %>% arrange(haul.id, length_cm)

# Replace NA density with 0 density because that's the added length-classes not previously in the catch data
datpla <- datpla %>% mutate(density = replace_na(density, 0))


# Dab
# Create a data frame with all combinations of trawl IDs and lengths
ex_df <- data.frame(expand.grid(
  length_cm = seq_range(datdab$length_cm, by = 1),
  haul.id = unique(datdab$haul.id))
  )

# Create an ID that is haul + length
ex_df$haul.id.size <- paste(ex_df$haul.id, ex_df$length_cm, sep = ".")
datdab$haul.id.size <- paste(datdab$haul.id, datdab$length_cm, sep = ".")

# Remove IDs that are already in datdab
ex_df <- ex_df %>% filter(!haul.id.size %in% unique(datdab$haul.id.size)) 

# Add in the other columns besides density and length
dat_for_join <- datdab %>% dplyr::select(-density, -length_cm, -haul.id.size) %>% distinct(haul.id, .keep_all = TRUE)

ex_df <- left_join(ex_df, dat_for_join, by = "haul.id")

# Bind_rows these data with datdab
datdab <- bind_rows(datdab, ex_df) %>% arrange(haul.id, length_cm)

# Replace NA density with 0 density because that's the added length-classes not previously in the catch data
datdab <- datdab %>% mutate(density = replace_na(density, 0))

# Merge all species data
dat <- bind_rows(datcod, datfle, datpla, datdab)

glimpse(dat)

# Check proportion zeroes
q1 <- dat %>% 
  filter(quarter == 1) %>% 
  group_by(species, haul.id) %>% 
  mutate(zero_catch = ifelse(sum(density) == 0, "Y", "N")) %>% 
  ungroup() %>% 
  group_by(year, species, zero_catch) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from = zero_catch, values_from = n) %>% 
  mutate(prop_z = Y / (N+Y), 
         q = 1)  

q4 <- dat %>% 
  filter(quarter == 4) %>% 
  group_by(species, haul.id) %>% 
  mutate(zero_catch = ifelse(sum(density) == 0, "Y", "N")) %>% 
  ungroup() %>% 
  group_by(year, species, zero_catch) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from = zero_catch, values_from = n) %>% 
  mutate(prop_z = Y / (N+Y), 
         q = 4)  

ggplot(bind_rows(q1, q4), aes(year, prop_z*100, color = factor(q))) +
  geom_line() +
  facet_wrap(~ species, ncol = 1)

# Save the full catch data as dat_full. For adding the unique covariates, I only need the distinct trawl IDs, not the 1.7 million rows...
# dat_full <- dat
# dat <- dat %>% dplyr::select(-density, length_cm, species, haul.id.size) %>% distinct(haul.id, .keep_all = TRUE)
# 
# # Check 0 catches
# dat_full %>%
#   filter(species == "cod") %>%
#   group_by(haul.id) %>%
#   summarise(haul_dens = sum(density)) %>%
#   ungroup() %>%
#   filter(!haul_dens == 0)
```

## Add in the environmental variables
### Substrate

```{r}
# substrate <- raster("data/substrate_tif/BALANCE_SEABED_SEDIMENT.tif")
# substrate_longlat = projectRaster(substrate, crs = ('+proj=longlat'))
# 
# # Now extract the values from the saduria raster to dat
# dat$substrate <- extract(substrate_longlat, dat %>% dplyr::select(lon, lat))
# 
# unique(dat$substrate)
# 
# # Plot
# ggplot(dat, aes(lon, lat, color = substrate)) + 
#   geom_point()
# 
# factor(sort(unique(round(dat$substrate))))
# 
# dat$substrate <- round(dat$substrate)
# 
# dat <- dat %>% mutate(substrate = ifelse(substrate == 1, "bedrock", substrate),
#                       substrate = ifelse(substrate == 2, "hard-bottom complex", substrate),
#                       substrate = ifelse(substrate == 3, "sand", substrate),
#                       substrate = ifelse(substrate == 4, "hard clay", substrate),
#                       substrate = ifelse(substrate == 5, "mud", substrate))
# # I. Bedrock.
# # II. Hard bottom complex, includes patchy hard surfaces and coarse sand (sometimes also clay) to boulders.
# # III. Sand including fine to coarse sand (with gravel exposures).
# # IV. Hard clay sometimes/often/possibly exposed or covered with a thin layer of
# # sand/gravel.
# # V. Mud including gyttja-clay to gyttja-silt.
# 
# # Plot
# ggplot(dat, aes(lon, lat, color = substrate)) + 
#   geom_point()
```

### Depth

```{r}
# Only use unique locations and then left_join else it will take forever
dat_haul <- dat %>% distinct(haul.id, .keep_all = TRUE) %>% dplyr::select(lat, lon, year, quarter)

# https://gis.stackexchange.com/questions/411261/read-multiple-layers-raster-from-ncdf-file-using-terra-package
# https://emodnet.ec.europa.eu/geoviewer/
dep_raster <- terra::rast("data/Mean depth natural colour (with land).nc")
class(dep_raster)
plot(dep_raster)

dat_haul$depth <- terra::extract(dep_raster, dat_haul %>% dplyr::select(lon, lat))$elevation

ggplot(dat_haul, aes(lon, lat, color = depth*-1)) + 
  geom_point() + 
  scale_color_viridis(direction = -1)

dat_haul$depth <- dat_haul$depth*-1
```

### Oxygen

```{r}
# Downloaded from here: https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=BALTICSEA_REANALYSIS_BIO_003_012
# Extract raster points: https://gisday.wordpress.com/2014/03/24/extract-raster-values-from-points-using-r/comment-page-1/
# https://rpubs.com/boyerag/297592
# https://pjbartlein.github.io/REarthSysSci/netCDF.html#get-a-variable
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-scobi-monthlymeans_1664182224542.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get oxygen
dname <- "o2b"

oxy_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(oxy_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
oxy_array[oxy_array == fillvalue$value] <- NA

# Next, we need to work with the months that correspond to the quarters that we use.
# loop through each time step, and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep_q1 <- which(months < 4)
index_keep_q4 <- which(months > 9)

oxy_q1 <- oxy_array[, , index_keep_q1]
oxy_q4 <- oxy_array[, , index_keep_q4]

months_keep_q1 <- months[index_keep_q1]
months_keep_q4 <- months[index_keep_q4]

years_keep_q1 <- years[index_keep_q1]
years_keep_q4 <- years[index_keep_q4]

# Now we have an array with data for that quarter
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq_q1 <- seq(1, dim(oxy_q1)[3], by = 3)
loop_seq_q4 <- seq(1, dim(oxy_q4)[3], by = 3)

# Create objects that will hold data
dlist_q1 <- list()
dlist_q4 <- list()

oxy_1 <- c()
oxy_2 <- c()
oxy_3 <- c()
oxy_ave_q1 <- c()

oxy_10 <- c()
oxy_11 <- c()
oxy_12 <- c()
oxy_ave_q4 <- c()

# Now average by quarter. The vector loop_seq_q1 is 1, 4, 7 etc. So first i is 1, 2, 3,
# which is the index we want. 

for(i in loop_seq_q1) { # We can use q1 as looping index, doesn't matter!
  
  oxy_1 <- oxy_q1[, , (i)]
  oxy_2 <- oxy_q1[, , (i + 1)]
  oxy_3 <- oxy_q1[, , (i + 2)]
  
  oxy_10 <- oxy_q4[, , (i)]
  oxy_11 <- oxy_q4[, , (i + 1)]
  oxy_12 <- oxy_q4[, , (i + 2)]
  
  oxy_ave_q1 <- (oxy_1 + oxy_2 + oxy_3) / 3
  oxy_ave_q4 <- (oxy_10 + oxy_11 + oxy_12) / 3
    
  list_pos_q1 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  list_pos_q4 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist_q1[[list_pos_q1]] <- oxy_ave_q1
  dlist_q4[[list_pos_q4]] <- oxy_ave_q4

}

# Now name the lists with the year:
names(dlist_q1) <- unique(years_keep_q1)
names(dlist_q4) <- unique(years_keep_q4)

# Now I need to make a loop where I extract the raster value for each year...
# The cpue data is called dat_haul so far in this script

# Filter years in the cpue data frame to only have the years I have oxygen for
d_sub_oxy_q1 <- dat_haul %>% filter(quarter == 1) %>% filter(year %in% names(dlist_q1)) %>% droplevels()
d_sub_oxy_q4 <- dat_haul %>% filter(quarter == 4) %>% filter(year %in% names(dlist_q4)) %>% droplevels()

# Create data holding object
oxy_data_list_q1 <- list()
oxy_data_list_q4 <- list()

# ... And for the oxygen raster
raster_list_q1 <- list()
raster_list_q4 <- list()

# Create factor year for indexing the list in the loop
d_sub_oxy_q1$year_f <- as.factor(d_sub_oxy_q1$year)
d_sub_oxy_q4$year_f <- as.factor(d_sub_oxy_q4$year)

# Loop through each year and extract raster values for the cpue data points
for(i in unique(d_sub_oxy_q1$year_f)) { # We can use q1 as looping index, doesn't matter!
  
  # Set plot limits
  ymin = 54; ymax = 58; xmin = 12; xmax = 22

  # Subset a year
  oxy_slice_q1 <- dlist_q1[[i]]
  oxy_slice_q4 <- dlist_q4[[i]]
  
  # Create raster for that year (i)
  r_q1 <- raster(t(oxy_slice_q1), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  r_q4 <- raster(t(oxy_slice_q4), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r_q1 <- flip(r_q1, direction = 'y')
  r_q4 <- flip(r_q4, direction = 'y')
  
  plot(r_q1, main = paste(i, "Q1"))
  plot(r_q4, main = paste(i, "Q4"))
  
  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice_q1 <- d_sub_oxy_q1 %>% filter(year_f == i) %>% dplyr::select(lon, lat)
  d_slice_q4 <- d_sub_oxy_q4 %>% filter(year_f == i) %>% dplyr::select(lon, lat)
  
  # Make into a SpatialPoints object
  data_sp_q1 <- SpatialPoints(d_slice_q1)
  data_sp_q4 <- SpatialPoints(d_slice_q4)
  
  # Extract raster value (oxygen)
  rasValue_q1 <- raster::extract(r_q1, data_sp_q1)
  rasValue_q4 <- raster::extract(r_q4, data_sp_q4)
  
  # Now we want to plot the results of the raster extractions by plotting the cpue
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for plot)
  df_q1 <- as.data.frame(data_sp_q1)
  df_q4 <- as.data.frame(data_sp_q4)
  
  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice_q1$oxy <- rasValue_q1
  d_slice_q4$oxy <- rasValue_q4
  
  # Add in which year
  d_slice_q1$year <- i
  d_slice_q4$year <- i

  # Now the unit of oxygen is mmol/m3. I want it to be ml/L. The original model is in unit ml/L
  # and it's been converted by the data host. Since it was converted without accounting for
  # pressure or temperature, I can simply use the following conversion factor:
  # 1 ml/l = 103/22.391 = 44.661 μmol/l -> 1 ml/l = 0.044661 mmol/l = 44.661 mmol/m^3 -> 0.0223909 ml/l = 1mmol/m^3
  # https://ocean.ices.dk/tools/unitconversion.aspx

  d_slice_q1$oxy <- d_slice_q1$oxy * 0.0223909
  d_slice_q4$oxy <- d_slice_q4$oxy * 0.0223909
    
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index_q1 <- as.numeric(as.character(d_slice_q1$year))[1] - 1992
  index_q4 <- as.numeric(as.character(d_slice_q4$year))[1] - 1992
  
  # Add each years' data in the list
  oxy_data_list_q1[[index_q1]] <- d_slice_q1
  oxy_data_list_q4[[index_q4]] <- d_slice_q4

}

# # Now create a data frame from the list of all annual values
big_dat_oxy_q1 <- dplyr::bind_rows(oxy_data_list_q1)
big_dat_oxy_q4 <- dplyr::bind_rows(oxy_data_list_q4)
big_dat_oxy <- bind_rows(mutate(big_dat_oxy_q1, quarter = 1),
                         mutate(big_dat_oxy_q4, quarter = 4))
```

### Temperature

```{r}
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-nemo-monthlymeans_1664183191233.nc")
                                        
print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get temperature
dname <- "bottomT"

temp_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(temp_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
temp_array[temp_array == fillvalue$value] <- NA

# Next, we need to work with the months that correspond to the quarters that we use.
# loop through each time step, and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep_q1 <- which(months < 4)
index_keep_q4 <- which(months > 9)

temp_q1 <- temp_array[, , index_keep_q1]
temp_q4 <- temp_array[, , index_keep_q4]

months_keep_q1 <- months[index_keep_q1]
months_keep_q4 <- months[index_keep_q4]

years_keep_q1 <- years[index_keep_q1]
years_keep_q4 <- years[index_keep_q4]

# Now we have an array with data for that quarter
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq_q1 <- seq(1, dim(temp_q1)[3], by = 3)
loop_seq_q4 <- seq(1, dim(temp_q4)[3], by = 3)

# Create objects that will hold data
dlist_q1 <- list()
dlist_q4 <- list()

temp_1 <- c()
temp_2 <- c()
temp_3 <- c()
temp_ave_q1 <- c()

temp_10 <- c()
temp_11 <- c()
temp_12 <- c()
temp_ave_q4 <- c()

# Now average by quarter. The vector loop_seq_q1 is 1, 4, 7 etc. So first i is 1, 2, 3,
# which is the index we want. 

for(i in loop_seq_q1) {
  
  temp_1 <- temp_q1[, , (i)]
  temp_2 <- temp_q1[, , (i + 1)]
  temp_3 <- temp_q1[, , (i + 2)]
  
  temp_10 <- temp_q4[, , (i)]
  temp_11 <- temp_q4[, , (i + 1)]
  temp_12 <- temp_q4[, , (i + 2)]
  
  temp_ave_q1 <- (temp_1 + temp_2 + temp_3) / 3
  temp_ave_q4 <- (temp_10 + temp_11 + temp_12) / 3
  
  list_pos_q1 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  list_pos_q4 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist_q1[[list_pos_q1]] <- temp_ave_q1
  dlist_q4[[list_pos_q4]] <- temp_ave_q4
  
}

# Now name the lists with the year:
names(dlist_q1) <- unique(years_keep_q1)
names(dlist_q4) <- unique(years_keep_q4)

# Now I need to make a loop where I extract the raster value for each year...
# The cpue data is called dat so far in this script

# Filter years in the cpue data frame to only have the years I have temperature for
d_sub_temp_q1 <- dat_haul %>% filter(quarter == 1) %>% filter(year %in% names(dlist_q1)) %>% droplevels()
d_sub_temp_q4 <- dat_haul %>% filter(quarter == 4) %>% filter(year %in% names(dlist_q4)) %>% droplevels()

# Create data holding object
temp_data_list_q1 <- list()
temp_data_list_q4 <- list()

# ... And for the temperature raster
raster_list_q1 <- list()
raster_list_q4 <- list()

# Create factor year for indexing the list in the loop
d_sub_temp_q1$year_f <- as.factor(d_sub_temp_q1$year)
d_sub_temp_q4$year_f <- as.factor(d_sub_temp_q4$year)

# Loop through each year and extract raster values for the cpue data points
for(i in unique(d_sub_temp_q1$year_f)) {
  
  # Set plot limits
  ymin = 54; ymax = 58; xmin = 12; xmax = 22
  
  # Subset a year
  temp_slice_q1 <- dlist_q1[[i]]
  temp_slice_q4 <- dlist_q4[[i]]
  
  # Create raster for that year (i)
  r_q1 <- raster(t(temp_slice_q1), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  r_q4 <- raster(t(temp_slice_q4), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r_q1 <- flip(r_q1, direction = 'y')
  r_q4 <- flip(r_q4, direction = 'y')
  
  plot(r_q1, main = paste(i, "Q1"))
  plot(r_q4, main = paste(i, "Q4"))
  
  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice_q1 <- d_sub_temp_q1 %>% filter(year_f == i) %>% dplyr::select(lon, lat)
  d_slice_q4 <- d_sub_temp_q4 %>% filter(year_f == i) %>% dplyr::select(lon, lat)
  
  # Make into a SpatialPoints object
  data_sp_q1 <- SpatialPoints(d_slice_q1)
  data_sp_q4 <- SpatialPoints(d_slice_q4)
  
  # Extract raster value (temperature)
  rasValue_q1 <- raster::extract(r_q1, data_sp_q1)
  rasValue_q4 <- raster::extract(r_q4, data_sp_q4)
  
  # Now we want to plot the results of the raster extractions by plotting the cpue
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df_q1 <- as.data.frame(data_sp_q1)
  df_q4 <- as.data.frame(data_sp_q4)
  
  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice_q1$temp <- rasValue_q1
  d_slice_q4$temp <- rasValue_q4
  
  # Add in which year
  d_slice_q1$year <- i
  d_slice_q4$year <- i
  
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index_q1 <- as.numeric(d_slice_q1$year)[1] - 1992
  index_q4 <- as.numeric(d_slice_q4$year)[1] - 1992
  
  # Add each years' data in the list
  temp_data_list_q1[[index_q1]] <- d_slice_q1
  temp_data_list_q4[[index_q4]] <- d_slice_q4
  
}

# Now create a data frame from the list of all annual values
big_dat_temp_q1 <- dplyr::bind_rows(temp_data_list_q1)
big_dat_temp_q4 <- dplyr::bind_rows(temp_data_list_q4)
big_dat_temp <- bind_rows(mutate(big_dat_temp_q1, quarter = 1),
                          mutate(big_dat_temp_q4, quarter = 4))
```

### Bottom salinity

```{r}
# https://data.marine.copernicus.eu/product/BALTICSEA_REANALYSIS_PHY_003_011/download?dataset=dataset-reanalysis-nemo-monthlymeans

# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-nemo-monthlymeans_1668587452211.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get Salinity
dname <- "sob"

sal_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(sal_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
sal_array[sal_array == fillvalue$value] <- NA

# Next, we need to work with the months that correspond to the quarters that we use.
# loop through each time step, and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep_q1 <- which(months < 4)
index_keep_q4 <- which(months > 9)

sal_q1 <- sal_array[, , index_keep_q1]
sal_q4 <- sal_array[, , index_keep_q4]

months_keep_q1 <- months[index_keep_q1]
months_keep_q4 <- months[index_keep_q4]

years_keep_q1 <- years[index_keep_q1]
years_keep_q4 <- years[index_keep_q4]

# Now we have an array with data for that quarter
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq_q1 <- seq(1, dim(sal_q1)[3], by = 3)
loop_seq_q4 <- seq(1, dim(sal_q4)[3], by = 3)

# Create objects that will hold data
dlist_q1 <- list()
dlist_q4 <- list()

sal_1 <- c()
sal_2 <- c()
sal_3 <- c()
sal_ave_q1 <- c()

sal_10 <- c()
sal_11 <- c()
sal_12 <- c()
sal_ave_q4 <- c()

# Now average by quarter. The vector loop_seq_q1 is 1, 4, 7 etc. So first i is 1, 2, 3,
# which is the index we want.

dim(sal_q1)
dim(sal_q4)

# Hmm, we didn't get the first month in the salinity series... repeat month 2 and fill in so the dimensions are correct
sal_q1 <- sal_q1[,,c(1, 1:83)]

dim(sal_q1)

for(i in loop_seq_q1) {

  sal_1 <- sal_q1[, , (i)]
  sal_2 <- sal_q1[, , (i + 1)]
  sal_3 <- sal_q1[, , (i + 2)]

  sal_10 <- sal_q4[, , (i)]
  sal_11 <- sal_q4[, , (i + 1)]
  sal_12 <- sal_q4[, , (i + 2)]

  sal_ave_q1 <- (sal_1 + sal_2 + sal_3) / 3
  sal_ave_q4 <- (sal_10 + sal_11 + sal_12) / 3

  list_pos_q1 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  list_pos_q4 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)

  dlist_q1[[list_pos_q1]] <- sal_ave_q1
  dlist_q4[[list_pos_q4]] <- sal_ave_q4

}

# Now name the lists with the year:
names(dlist_q1) <- unique(years_keep_q1)
names(dlist_q4) <- unique(years_keep_q4)

# Now I need to make a loop where I extract the raster value for each year...
# The cpue data is called dat so far in this script

# Filter years in the cpue data frame to only have the years I have salinity for
d_sub_sal_q1 <- dat_haul %>% filter(quarter == 1) %>% filter(year %in% names(dlist_q1)) %>% droplevels()
d_sub_sal_q4 <- dat_haul %>% filter(quarter == 4) %>% filter(year %in% names(dlist_q4)) %>% droplevels()

# Create data holding object
sal_data_list_q1 <- list()
sal_data_list_q4 <- list()

# ... And for the salinity raster
raster_list_q1 <- list()
raster_list_q4 <- list()

# Create factor year for indexing the list in the loop
d_sub_sal_q1$year_f <- as.factor(d_sub_sal_q1$year)
d_sub_sal_q4$year_f <- as.factor(d_sub_sal_q4$year)

# Loop through each year and extract raster values for the cpue data points
for(i in unique(d_sub_sal_q1$year_f)) {

  # Set plot limits
  ymin = 54; ymax = 58; xmin = 12; xmax = 22

  # Subset a year
  sal_slice_q1 <- dlist_q1[[i]]
  sal_slice_q4 <- dlist_q4[[i]]

  # Create raster for that year (i)
  r_q1 <- raster(t(sal_slice_q1), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  r_q4 <- raster(t(sal_slice_q4), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))

  # Flip...
  r_q1 <- flip(r_q1, direction = 'y')
  r_q4 <- flip(r_q4, direction = 'y')

  plot(r_q1, main = paste(i, "Q1"))
  plot(r_q4, main = paste(i, "Q4"))

  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice_q1 <- d_sub_sal_q1 %>% filter(year_f == i) %>% dplyr::select(lon, lat)
  d_slice_q4 <- d_sub_sal_q4 %>% filter(year_f == i) %>% dplyr::select(lon, lat)

  # Make into a SpatialPoints object
  data_sp_q1 <- SpatialPoints(d_slice_q1)
  data_sp_q4 <- SpatialPoints(d_slice_q4)

  # Extract raster value (salinity)
  rasValue_q1 <- raster::extract(r_q1, data_sp_q1)
  rasValue_q4 <- raster::extract(r_q4, data_sp_q4)

  # Now we want to plot the results of the raster extractions by plotting the cpue
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df_q1 <- as.data.frame(data_sp_q1)
  df_q4 <- as.data.frame(data_sp_q4)

  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice_q1$sal <- rasValue_q1
  d_slice_q4$sal <- rasValue_q4

  # Add in which year
  d_slice_q1$year <- i
  d_slice_q4$year <- i

  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index_q1 <- as.numeric(d_slice_q1$year)[1] - 1992
  index_q4 <- as.numeric(d_slice_q4$year)[1] - 1992

  # Add each years' data in the list
  sal_data_list_q1[[index_q1]] <- d_slice_q1
  sal_data_list_q4[[index_q4]] <- d_slice_q4

}

# Now create a data frame from the list of all annual values
big_dat_sal_q1 <- dplyr::bind_rows(sal_data_list_q1)
big_dat_sal_q4 <- dplyr::bind_rows(sal_data_list_q4)
big_dat_sal <- bind_rows(mutate(big_dat_sal_q1, quarter = 1),
                          mutate(big_dat_sal_q4, quarter = 4))
```

```{r merge oxygen temp and salinity data with fish data}
env_dat <- left_join(big_dat_oxy %>% mutate(id_env = paste(year, quarter, lon, lat, sep = ".")),
                     big_dat_temp %>% mutate(id_env = paste(year, quarter, lon, lat, sep = ".")) %>% 
                       dplyr::select(temp, id_env))

env_dat <- left_join(env_dat,
                     big_dat_sal %>%
                       mutate(id_env = paste(year, quarter, lon, lat, sep = ".")) %>% 
                       dplyr::select(sal, id_env))

env_dat <- left_join(env_dat,
                     dat_haul %>% mutate(id_env = paste(year, quarter, lon, lat, sep = ".")) %>% 
                       dplyr::select(depth, id_env))

# Now join these data with the full_dat
dat_full <- left_join(dat %>% mutate(id_env = paste(year, quarter, lon, lat, sep = ".")),
                      env_dat %>% dplyr::select(id_env, oxy, temp, sal, depth))
```

## Add UTM coords

```{r}
# First add UTM coords
# Add UTM coords

utm_coords <- LongLatToUTM(dat_full$lon, dat_full$lat, zone = 33)
dat_full$X <- utm_coords$X/1000 # for computational reasons
dat_full$Y <- utm_coords$Y/1000 # for computational reasons
```

## Save data

```{r}
dat_full_save <- dat_full %>%
  dplyr::select(-IDx, -id_env, -haul.id.size) %>% 
  janitor::clean_names()

write.csv(dat_full_save, file = "data/clean/catch_by_length_q1_q4.csv", row.names = FALSE)
```

# read and summarize in new script
